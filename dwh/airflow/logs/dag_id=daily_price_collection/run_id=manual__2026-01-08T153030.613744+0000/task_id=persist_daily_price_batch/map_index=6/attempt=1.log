[2026-01-08T15:36:31.178+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2026-01-08T15:36:31.200+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: daily_price_collection.persist_daily_price_batch manual__2026-01-08T15:30:30.613744+00:00 map_index=6 [queued]>
[2026-01-08T15:36:31.213+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: daily_price_collection.persist_daily_price_batch manual__2026-01-08T15:30:30.613744+00:00 map_index=6 [queued]>
[2026-01-08T15:36:31.214+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2026-01-08T15:36:31.233+0000] {taskinstance.py:2330} INFO - Executing <Mapped(_PythonDecoratedOperator): persist_daily_price_batch> on 2026-01-08 15:30:30.613744+00:00
[2026-01-08T15:36:31.245+0000] {warnings.py:112} WARNING - This process (pid=4565) is multi-threaded, use of fork() may lead to deadlocks in the child.

[2026-01-08T15:36:31.247+0000] {standard_task_runner.py:64} INFO - Started process 4679 to run task
[2026-01-08T15:36:31.249+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'daily_price_collection', 'persist_daily_price_batch', 'manual__2026-01-08T15:30:30.613744+00:00', '--job-id', '7011', '--raw', '--subdir', 'DAGS_FOLDER/daily_price_collection.py', '--cfg-path', '/tmp/tmp_1kcydmk', '--map-index', '6']
[2026-01-08T15:36:31.251+0000] {standard_task_runner.py:91} INFO - Job 7011: Subtask persist_daily_price_batch
[2026-01-08T15:36:31.637+0000] {task_command.py:426} INFO - Running <TaskInstance: daily_price_collection.persist_daily_price_batch manual__2026-01-08T15:30:30.613744+00:00 map_index=6 [running]> on host 7568acf023da
[2026-01-08T15:36:31.758+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 2479, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 2633, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context, jinja_env=jinja_env)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3094, in render_templates
    original_task.render_template_fields(context, jinja_env)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/mappedoperator.py", line 830, in render_template_fields
    unmapped_task = self.unmap(mapped_kwargs)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/mappedoperator.py", line 747, in unmap
    op = self.operator_class(**kwargs, _airflow_from_mapped=True)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 485, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/python.py", line 52, in __init__
    super().__init__(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 485, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 253, in __init__
    signature.bind(*op_args, **op_kwargs)
  File "/usr/local/lib/python3.12/inspect.py", line 3267, in bind
    return self._bind(args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/inspect.py", line 3180, in _bind
    raise TypeError(msg) from None
TypeError: missing a required argument: 'symbols'
[2026-01-08T15:36:31.779+0000] {taskinstance.py:2953} ERROR - Unable to unmap task to determine if we need to send an alert email
[2026-01-08T15:36:31.781+0000] {taskinstance.py:1206} INFO - Marking task as UP_FOR_RETRY. dag_id=daily_price_collection, task_id=persist_daily_price_batch, run_id=manual__2026-01-08T15:30:30.613744+00:00, map_index=6, execution_date=20260108T153030, start_date=20260108T153631, end_date=20260108T153631
[2026-01-08T15:36:31.809+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 7011 for task persist_daily_price_batch (missing a required argument: 'symbols'; 4679)
[2026-01-08T15:36:31.865+0000] {local_task_job_runner.py:243} INFO - Task exited with return code 1
[2026-01-08T15:36:32.327+0000] {taskinstance.py:3509} WARNING - Skipping mini scheduling run due to exception: SELECT task_instance.try_number, task_instance.task_display_name, task_instance.task_id, task_instance.dag_id, task_instance.run_id, task_instance.map_index, task_instance.start_date, task_instance.end_date, task_instance.duration, task_instance.state, task_instance.max_tries, task_instance.hostname, task_instance.unixname, task_instance.job_id, task_instance.pool, task_instance.pool_slots, task_instance.queue, task_instance.priority_weight, task_instance.operator, task_instance.custom_operator_name, task_instance.queued_dttm, task_instance.queued_by_job_id, task_instance.pid, task_instance.executor_config, task_instance.updated_at, task_instance.rendered_map_index, task_instance.external_executor_id, task_instance.trigger_id, task_instance.trigger_timeout, task_instance.next_method, task_instance.next_kwargs, dag_run_1.state AS state_1, dag_run_1.id, dag_run_1.dag_id AS dag_id_1, dag_run_1.queued_at, dag_run_1.execution_date, dag_run_1.start_date AS start_date_1, dag_run_1.end_date AS end_date_1, dag_run_1.run_id AS run_id_1, dag_run_1.creating_job_id, dag_run_1.external_trigger, dag_run_1.run_type, dag_run_1.conf, dag_run_1.data_interval_start, dag_run_1.data_interval_end, dag_run_1.last_scheduling_decision, dag_run_1.dag_hash, dag_run_1.log_template_id, dag_run_1.updated_at AS updated_at_1, dag_run_1.clear_number 
FROM task_instance JOIN dag_run AS dag_run_1 ON dag_run_1.dag_id = task_instance.dag_id AND dag_run_1.run_id = task_instance.run_id 
WHERE task_instance.dag_id = %(dag_id_2)s AND task_instance.run_id = %(run_id_2)s AND (task_instance.state IN (%(state_2_1)s, %(state_2_2)s, %(state_2_3)s, %(state_2_4)s, %(state_2_5)s, %(state_2_6)s, %(state_2_7)s, %(state_2_8)s, %(state_2_9)s, %(state_2_10)s, %(state_2_11)s, %(state_2_12)s, %(state_2_13)s) OR task_instance.state IS NULL) AND task_instance.task_id IN (NULL) AND (1 != 1)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.OperationalError: server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3481, in _schedule_downstream_tasks
    info = dag_run.task_instance_scheduling_decisions(session)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagrun.py", line 930, in task_instance_scheduling_decisions
    tis = self.get_task_instances(session=session, state=State.task_states)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagrun.py", line 625, in get_task_instances
    return DagRun.fetch_task_instances(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 115, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagrun.py", line 562, in fetch_task_instances
    return session.scalars(tis).all()
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.

[SQL: SELECT task_instance.try_number, task_instance.task_display_name, task_instance.task_id, task_instance.dag_id, task_instance.run_id, task_instance.map_index, task_instance.start_date, task_instance.end_date, task_instance.duration, task_instance.state, task_instance.max_tries, task_instance.hostname, task_instance.unixname, task_instance.job_id, task_instance.pool, task_instance.pool_slots, task_instance.queue, task_instance.priority_weight, task_instance.operator, task_instance.custom_operator_name, task_instance.queued_dttm, task_instance.queued_by_job_id, task_instance.pid, task_instance.executor_config, task_instance.updated_at, task_instance.rendered_map_index, task_instance.external_executor_id, task_instance.trigger_id, task_instance.trigger_timeout, task_instance.next_method, task_instance.next_kwargs, dag_run_1.state AS state_1, dag_run_1.id, dag_run_1.dag_id AS dag_id_1, dag_run_1.queued_at, dag_run_1.execution_date, dag_run_1.start_date AS start_date_1, dag_run_1.end_date AS end_date_1, dag_run_1.run_id AS run_id_1, dag_run_1.creating_job_id, dag_run_1.external_trigger, dag_run_1.run_type, dag_run_1.conf, dag_run_1.data_interval_start, dag_run_1.data_interval_end, dag_run_1.last_scheduling_decision, dag_run_1.dag_hash, dag_run_1.log_template_id, dag_run_1.updated_at AS updated_at_1, dag_run_1.clear_number 
FROM task_instance JOIN dag_run AS dag_run_1 ON dag_run_1.dag_id = task_instance.dag_id AND dag_run_1.run_id = task_instance.run_id 
WHERE task_instance.dag_id = %(dag_id_2)s AND task_instance.run_id = %(run_id_2)s AND (task_instance.state IN (%(state_2_1)s, %(state_2_2)s, %(state_2_3)s, %(state_2_4)s, %(state_2_5)s, %(state_2_6)s, %(state_2_7)s, %(state_2_8)s, %(state_2_9)s, %(state_2_10)s, %(state_2_11)s, %(state_2_12)s, %(state_2_13)s) OR task_instance.state IS NULL) AND task_instance.task_id IN (NULL) AND (1 != 1)]
[parameters: {'dag_id_2': 'daily_price_collection', 'run_id_2': 'manual__2026-01-08T15:30:30.613744+00:00', 'state_2_1': <TaskInstanceState.REMOVED: 'removed'>, 'state_2_2': <TaskInstanceState.SCHEDULED: 'scheduled'>, 'state_2_3': <TaskInstanceState.QUEUED: 'queued'>, 'state_2_4': <TaskInstanceState.RUNNING: 'running'>, 'state_2_5': <TaskInstanceState.SUCCESS: 'success'>, 'state_2_6': <TaskInstanceState.RESTARTING: 'restarting'>, 'state_2_7': <TaskInstanceState.FAILED: 'failed'>, 'state_2_8': <TaskInstanceState.UP_FOR_RETRY: 'up_for_retry'>, 'state_2_9': <TaskInstanceState.UP_FOR_RESCHEDULE: 'up_for_reschedule'>, 'state_2_10': <TaskInstanceState.UPSTREAM_FAILED: 'upstream_failed'>, 'state_2_11': <TaskInstanceState.SKIPPED: 'skipped'>, 'state_2_12': <TaskInstanceState.DEFERRED: 'deferred'>, 'state_2_13': <TaskInstanceState.SHUTDOWN: 'shutdown'>}]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2026-01-08T15:36:32.348+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
